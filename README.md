# AI-software
- Unet就是典型的编码器-解码器，不断采样使得通道数翻倍，分辨率降低。然后再反过来，Unet的拼接能显著改善边缘于小目标。

- 双分支如果强行改成单分支会出现早起混合融合的情况，就算你只是将卷积核复制了一层，可是输出层都是读取两个通道再加权求和的

- 重要性感知混合精度量化：首先是经过重要性度量（赋值大小、梯度敏感度（二阶导）、通道缩放系数）

- 之所以采用蒸馏是因为参数量越大能学习到的东西越多，同时小模型训练需要极高的训练数据质量； 原始标签告诉你“答案是猫”， 老师模型告诉你“90% 猫，5% 狐狸，5% 狗”

- Flash Attention 是一种优化的注意力机制，本质就是减少kv在显存中的占用，采用一种在线算法

- 多头注意力机制将输入分成多个"头"，每个头独立地执行自注意力计算，然后将所有头的输出合并起来。每个头可以关注输入序列的不同方面，从而捕获更丰富的特征信息； 本质就是不同的head有不同的w

- Multi-Query Attention 是多个Query共享同一份k和v，只是保存自己独特的Q； 也就是说MHA是有多个QKV，而MQA是多个Q共享kv； GQA就是一种折中，组内共享kv，方便部署

- MoE通过动态路由算法 (如Top-K Gating) , 每个输入样本仅激活少量专家 (比如DeepSeek-R1每个token仅激活8个专家) , 来达到减少计算量的目的，就是替换将Transformer 里面的FFN替换成 MoE

- LoRA就是将权重矩阵分解成一个大矩阵和一个低秩矩阵，然后对这个低秩矩阵进行调整

- 微调是让模型在某个特定领域表现更好，需要长期执行任务，提示词不够用等； 如果任务变化快，数据 不足等那就直接使用RAG

- 
